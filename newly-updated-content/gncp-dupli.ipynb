{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "from pickle import load\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers.pop()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'description' in link mlmastery = 'mapping' in this code which are dictionaries of {id:[desc1,desc2,desc3]}\n",
    "def inputimage(directory):\n",
    "    features = dict()\n",
    "    processed_imagescount = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        comp_filename = directory+\"/\"+filename\n",
    "        \n",
    "        print(comp_filename)\n",
    "        image=load_img(comp_filename,target_size=(224,224))\n",
    "        \n",
    "        image=img_to_array(image)\n",
    "        \n",
    "        print(image.shape,image.size)\n",
    "        image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "        \n",
    "        print(image.shape,image.size)\n",
    "        image=preprocess_input(image)\n",
    "        \n",
    "        print(image.shape,image.size)\n",
    "#\n",
    "        filename = filename.split('.')[0]\n",
    "    \n",
    "        prediction = model.predict(image,verbose=0)\n",
    "        \n",
    "        processed_imagescount+=1\n",
    "        \n",
    "        print('processed_imagescount = ',processed_imagescount)\n",
    "    \n",
    "        features[filename] = prediction\n",
    "    \n",
    "        #print(type(prediction))\n",
    "        #print(prediction.shape)\n",
    "        #print(prediction.argmax(axis=1))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8091\n"
     ]
    }
   ],
   "source": [
    "h=0\n",
    "directory = \"Flicker8k_Dataset\"\n",
    "for filename in os.listdir(directory):\n",
    "    h+=1 \n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS.ipynb             Untitled2.ipynb      model.png\r\n",
      "\u001b[34mFlicker8k_Dataset\u001b[m\u001b[m/   cleanedcaptions.txt  sele.ipynb\r\n",
      "\u001b[34mFlickr8k_text\u001b[m\u001b[m/       \u001b[34mfakeimg\u001b[m\u001b[m/             submission.csv\r\n",
      "Newint.ipynb         features.pkl         tokenizer.pkl\r\n",
      "Untitled.ipynb       gncp.ipynb\r\n",
      "Untitled1.ipynb      gncp2.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndirectory = \"Flicker8k_Dataset\"#\"Flicker8k_Dataset\"\\nfeatures = inputimage(directory)\\n#saving features\\ndump(features,open(\"features.pkl\",\"wb\"))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "directory = \"Flicker8k_Dataset\"#\"Flicker8k_Dataset\"\n",
    "features = inputimage(directory)\n",
    "#saving features\n",
    "dump(features,open(\"features.pkl\",\"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all above\n",
    "features = load(open('features.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loading data from Flickr8k.token.txt\n",
    "def load_data(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "filename = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "data = load_data(filename)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping image and all its captions\n",
    "\n",
    "def mapping(data):\n",
    "    img_data_map = dict()\n",
    "    t=0\n",
    "    for line in data.split(\"\\n\")[:-1]:#iterate till 2nd last element\n",
    "        t=t+1\n",
    "        #print(line)\n",
    "        token = line.split()\n",
    "        #print(token)\n",
    "        img_id = token[0]\n",
    "        img_desc = token[1:] \n",
    "        #print(img_id,\" -> \",img_desc)\n",
    "        img_desc = ' '.join(img_desc)\n",
    "\n",
    "        img_id_ = img_id.split('.')[0]\n",
    "        #print(img_id_)\n",
    "        print(img_id_,img_desc)        \n",
    "        if img_id_ not in img_data_map:\n",
    "            img_data_map[img_id_]=list()\n",
    "        img_data_map[img_id_].append(img_desc)\n",
    "        \n",
    "    return img_data_map,t\n",
    "t=0\n",
    "mapping,t = mapping(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(imgid_data_map))\n",
    "for i in mapping:\n",
    "    print(i,\"  \",mapping[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "def clean_descriptions(mapping):\n",
    "    for i in mapping:\n",
    "        #lowercase\n",
    "        for j in range(0,len(mapping[i])):\n",
    "            #print(type(mapping[i]))\n",
    "            #print(type(mapping[i][j]))\n",
    "            print(mapping[i][j])\n",
    "            mapping[i][j]=mapping[i][j].lower();\n",
    "            mapping[i][j].translate(str.maketrans('', '', string.punctuation))\n",
    "            mapping[i][j]=' '.join([w for w in mapping[i][j].split() if len(w)>1])\n",
    "            mapping[i][j]=' '.join([w for w in mapping[i][j].split() if w.isalpha()])\n",
    "    '''    \n",
    "    '''\n",
    "mapping = clean_descriptions(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdf sd df'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 'sdf  , sd 100 df'\n",
    "#' '.join(w for w in mapping[i][j] if w.isalpha())\n",
    "' '.join([w for w in b.split() if w.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(mapping):\n",
    "    vocab=set()\n",
    "    for i in mapping:\n",
    "        for j in range(0,len(mapping[i])):\n",
    "            words=mapping[i][j].split()\n",
    "            for k in words:\n",
    "                vocab.add(k)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=build_vocab(mapping)\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mapping(mapping,filename):\n",
    "    element=[]\n",
    "    tot=0\n",
    "    for img,descs in mapping.items():\n",
    "        for desc in descs:\n",
    "            tot=tot+1\n",
    "            element.append(img+' '+desc)\n",
    "    data='\\n'.join(element)\n",
    "    #print(element)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    return tot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_descs=save_mapping(mapping,'cleanedcaptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40460"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8357"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the img without extension\n",
    "def filename(filename):\n",
    "    files=set()\n",
    "    file = open(filename,'r')\n",
    "    text = file.read();\n",
    "    file.close()\n",
    "    \n",
    "    lst = list()\n",
    "    lst=text.split('\\n')\n",
    "    for i in range(0,len(lst)):\n",
    "        #print(lst[i],'\\n')\n",
    "        lst[i] = lst[i].split('.')[0]\n",
    "        print(lst[i],'\\n')\n",
    "        \n",
    "    return lst\n",
    "    #return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id=filename('Flickr8k_text/Flickr_8k.trainImages.txt')\n",
    "len(train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flickr8k_text/Flickr_8k.trainImages.txt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Flickr8k_text/Flickr_8k.trainImages.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#return dict of a \n",
    "{\n",
    "    image_id : [---,---,---]\n",
    "}\n",
    "#for those img_id present in training set. Same can be done for development set ..\n",
    "\n",
    "'''\n",
    "def load_details(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "#descriptions.txt = cleanedcaptions.txt\n",
    "\n",
    "\n",
    "# We have the train.txt file which contains the imageid of train images. \n",
    "#We make a dictionary named \"specific_clean_desc\" and we take each image id  from the description.txt which we\n",
    "#saved previously which contains \"id : caption\" pair. id can be repeated bcz a single id has may captions.\n",
    "#We loop through each of these pair and if any one of these is present in the train_id then we add 'startseq' and 'endseq'\n",
    "#in the caption(from description.txt) and make a {id1:[cap1,cap2,cap3], id2:[cap1,cap2,cap3]} and  return this dictionary.\n",
    "\n",
    "def load_desc(cleanedcaptions,dataset):\n",
    "    specific_clean_desc = dict()\n",
    "    id_desc_pair = load_details(cleanedcaptions)\n",
    "    for line in id_desc_pair.split('\\n'):\n",
    "        #print(line)\n",
    "        imgid , desc = line.split()[0] , line.split()[1:]\n",
    "#        print(type(desc))\n",
    "        if imgid in dataset:\n",
    "#            print(\"k\")\n",
    "            newdesc = 'startseq ' + ' '.join(desc) + ' endseq'\n",
    "#            print(newdesc)\n",
    "            if imgid not in specific_clean_desc:\n",
    "#                print(\"not\")\n",
    "                specific_clean_desc[imgid] = list()\n",
    "            specific_clean_desc[imgid].append(newdesc)\n",
    "            #print(specific_clean_desc[imgid])\n",
    "    return specific_clean_desc\n",
    "train_descs = load_desc(\"cleanedcaptions.txt\",train_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_descs))\n",
    "train_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "#checking that all of the train images are present in the description.txt as ids.\n",
    "x=0\n",
    "for key in train_descs:\n",
    "    if key in train_id:\n",
    "        #print('uea')\n",
    "        x=x+1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_id))\n",
    "#reloading the features from feature.pkl\n",
    "#We saved the 4096 dimensional vector for each image in the the entire images dataset(contained the images).\n",
    "#features relevant to the train dataset is extracted\n",
    "from pickle import load\n",
    "def load_features(pklfile, train_imgid):\n",
    "    \n",
    "    features_vector = load(open(pklfile,'rb'))#load(filehandler) is used to load a given dumped file\n",
    "    #print(len(all_features))\n",
    "    #print(type(all_features))\n",
    "    train_features = dict()\n",
    "    \n",
    "    for imgid in train_imgid:\n",
    "        #print('>>',imgid)\n",
    "        #print(len(all_features[imgid]))\n",
    "        #print(train_features[imgid]) #\n",
    "        train_features.update({ imgid : features_vector[imgid] })\n",
    "        print(features_vector[imgid])\n",
    "        #adding features of train image ids to a dictionary\n",
    "\n",
    "    return train_features\n",
    "                    \n",
    "\n",
    "train_features = load_features('features.pkl',train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a dictionary of {id1:[cap1,cap2,cap3], id2:[cap1,cap2,cap3]} (this we made for the train set where ids \n",
    "#are train set images id and different captions associated with an id), create a list of all the caps. ie [cap1,cap2,cap3]\n",
    "#refer for https://github.com/keras-team/keras/issues/7551\n",
    "def to_list(dictof_train_idcap):\n",
    "\n",
    "    text_descs = list()\n",
    "    \n",
    "    for ids in dictof_train_idcap:\n",
    "        for caption in dictof_train_idcap[ids]:\n",
    "            text_descs.append(caption)\n",
    "    return text_descs\n",
    "\n",
    "def create_tokenizer(dictof_train_idcap):\n",
    "    lines = to_list(dictof_train_idcap)\n",
    "#    print(type(lines))\n",
    "#    print(lines)\n",
    " \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return(tokenizer)\n",
    " \n",
    "        \n",
    "tokenizer = create_tokenizer(train_descs)\n",
    "#print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x1a372e1a90>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8357\n"
     ]
    }
   ],
   "source": [
    "#total vocab size is 8357 and train vocab size is 7266 which means that training set caption has lesser no. of words\n",
    "#and we may see unseen words in test, valid sets\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7266"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab_size  = len(tokenizer.word_index) + 1\n",
    "#to check why 1 is added got to https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ check under \n",
    "#2. Keras Embedding Layer\n",
    "train_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX1,\\t\\tX2 (text sequence), \\t\\t\\t\\t\\t\\ty (word)\\nphoto\\tstartseq, \\t\\t\\t\\t\\t\\t\\t\\t\\tlittle\\nphoto\\tstartseq, little,\\t\\t\\t\\t\\t\\t\\tgirl\\nphoto\\tstartseq, little, girl, \\t\\t\\t\\t\\trunning\\nphoto\\tstartseq, little, girl, running, \\t\\t\\tin\\nphoto\\tstartseq, little, girl, running, in, \\t\\tfield\\nphoto\\tstartseq, little, girl, running, in, field, endseq\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X1,\t\tX2 (text sequence), \t\t\t\t\t\ty (word)\n",
    "photo\tstartseq, \t\t\t\t\t\t\t\t\tlittle\n",
    "photo\tstartseq, little,\t\t\t\t\t\t\tgirl\n",
    "photo\tstartseq, little, girl, \t\t\t\t\trunning\n",
    "photo\tstartseq, little, girl, running, \t\t\tin\n",
    "photo\tstartseq, little, girl, running, in, \t\tfield\n",
    "photo\tstartseq, little, girl, running, in, field, endseq\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "def maxlensentence(train):\n",
    "    mx = 0\n",
    "    for id in train:\n",
    "        for sentence in train[id]:\n",
    "            #print(type(sentence))\n",
    "            #print(type(sentence.split()))\n",
    "            mx= max(mx,len(sentence.split()))\n",
    "    return mx\n",
    "maxlen = maxlensentence(train_descs)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it iterates over all the train_descs(which contain {id:[desc1,desc2,desc3]} of only train dataset)\n",
    "# and for each for each id we pick each of its descriptions and divide it into\n",
    "#\"multiple input sequences\" and the predicted sequence.\n",
    "#eg - [1,2,3,4,5]\n",
    "#[1],[2]\n",
    "#[1,2], [3]\n",
    "#[1,2,3], [4]\n",
    "##[1,2,3,4], [5]\n",
    "# we need:\n",
    "#the tokenizer to convert each word to a number\n",
    "#feature_vector(4096) #train_features is a dictionary and not a list. {id:[4096 element],id:[4096 element]}\n",
    "#dictionary containing the train {id:[desc1,desc2,desc3]} which is used to loop through\n",
    "#maxlen to pad the smaller sentences\n",
    "#mapping contain mapping of all ids and description in {id:[desc1,desc2,desc3]} format.\n",
    "def create_sequences(tokenizer,train_descs,train_features,maxlen):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, descs in train_descs.items():\n",
    "        for desc in descs:\n",
    "            \n",
    "            #print(type(desc))\n",
    "            seqofnumber = tokenizer.texts_to_sequences([desc])[0]\n",
    "            #print('seqofnumber = ',seqofnumber)\n",
    "\n",
    "            \n",
    "            for i in range(1,len(seqofnumber)):##edit : len(seqofnumber)-1\n",
    "                inseq, outseq = seqofnumber[0:i], seqofnumber[i]\n",
    "                print('\\t',inseq,outseq)\n",
    "                inseq = pad_sequences([inseq],maxlen = maxlen)[0]\n",
    "                outseq = to_categorical([outseq],num_classes = train_vocab_size)[0]\n",
    "                print('outseq',outseq.shape)\n",
    "                print(key)\n",
    "                X1.append(train_features[key][0])#train_features is a dictionary and not a list.\n",
    "                #train_features = {'id' : [], 'id': []}\n",
    "                X2.append(inseq)\n",
    "                y.append(outseq)\n",
    "                #print(\"\\ny :\",y.shape)\n",
    "                \n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer,train_descs,train_features,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical([3],num_classes = 5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305617, 7266)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[3], [4], [1], [], [1], [2], [8], [3], [4], [], [5], [6], [], [2], [9], [], [], [8], [1], [2], [3], [], [13], [7], [2], [14], [1], [], [7], [5], [15], [1]] \n",
      "\n",
      "word_index :  {'e': 1, 'a': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'l': 7, 'r': 8, 'n': 9, 'w': 10, 'o': 11, 'm': 12, 'p': 13, 'c': 14, 'v': 15}\n"
     ]
    }
   ],
   "source": [
    "t  = Tokenizer()\n",
    "fit_text = \"The earth is an awesome place live\"\n",
    "t.fit_on_texts(fit_text)\n",
    "test_text = \"The earth is an great place live\"\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"sequences : \",sequences,'\\n')\n",
    "\n",
    "print(\"word_index : \",t.word_index)\n",
    "#[] specifies : 1. space b/w the words in the test_text    2. letters that have not occured in fit_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[1, 2, 3, 4, 6, 7], [1, 3]] \n",
      "\n",
      "word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t  = Tokenizer()\n",
    "fit_text = [\"The earth is an awesome place live\"]\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "#fit_on_texts fits on sentences when list of sentences is passed to fit_on_texts() function. \n",
    "#ie - fit_on_texts( [ sent1, sent2, sent3,....sentN ] )\n",
    "\n",
    "#Similarly, list of sentences/single sentence in a list must be passed into texts_to_sequences.\n",
    "test_text1 = \"The earth is an great place live\"\n",
    "test_text2 = \"The is my program\"\n",
    "sequences = t.texts_to_sequences([test_text1, test_text2])\n",
    "\n",
    "print('sequences : ',sequences,'\\n')\n",
    "\n",
    "print('word_index : ',t.word_index)\n",
    "#texts_to_sequences() returns list of list. ie - [ [] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex1: [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]] \n",
      "\n",
      "Ex2: \n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "l = [0, 2, 1, 2, 0]\n",
    "print('Ex1:',to_categorical(l),'\\n\\nEx2: ')\n",
    "print(to_categorical(l,num_classes = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Embedding(7, 2, input_length=5)\n",
    "\n",
    "#The first argument (7) is the number of distinct words in the training set. \n",
    "#The second argument (2) indicates the size of the embedding vectors.\n",
    "#The input_length argumet, of course, determines the size of each input sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_id=filename('Flickr8k_text/Flickr_8k.devImages.txt')#loadpath\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc = load_desc(\"cleanedcaptions.txt\",test_id)\n",
    "#test_features = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = load_features('features.pkl',test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1test, X2test, ytest = create_sequences(tokenizer, test_desc, test_features, maxlen )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(vocab_size, max_length):\n",
    "    \n",
    "    input1_vgg = Input(shape = (4096, ))\n",
    "    dropout_vgg = Dropout(0.5)(input1_vgg)\n",
    "    dense_vgg = Dense(256, activation = 'relu')(dropout_vgg)\n",
    "    \n",
    "    input2_word = Input(shape = (max_length, ))\n",
    "    embed_word = Embedding(vocab_size, 256, mask_zero = True)(input2_word)\n",
    "    dropout_word = Dropout(0.5)(embed_word)\n",
    "    lstm_word = LSTM(256)(dropout_word)\n",
    "    \n",
    "    adder = add([dense_vgg,lstm_word])\n",
    "    dense = Dense(256, activation='relu')(adder)\n",
    "    \n",
    "    final = Dense(vocab_size, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(input=[input1_vgg,input2_word], output=[final])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajay/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 33, 256)      1860096     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 33, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7266)         1867362     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,367,394\n",
      "Trainable params: 5,367,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model(train_vocab_size, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(([X1train,X2train]),ytrain,epochs=3,verbose=2,callbacks=[checkpoint], validation_data=([X1test,X2test],ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1train.shape:  (305617, 4096) \n",
      "\n",
      "X2train.shape:  (305617, 33) \n",
      "\n",
      "ytrain.shape:  (305617, 7266) \n",
      "\n",
      "X1test.shape:  (50835, 4096) \n",
      "\n",
      "X2test.shape:  (50835, 33) \n",
      "\n",
      "ytest.shape:  (305617, 7266) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X1train.shape: ',X1train.shape,'\\n')\n",
    "print('X2train.shape: ',X2train.shape,'\\n')\n",
    "print('ytrain.shape: ',ytrain.shape,'\\n')\n",
    "\n",
    "print('X1test.shape: ',X1test.shape,'\\n')\n",
    "print('X2test.shape: ',X2test.shape,'\\n')\n",
    "print('ytest.shape: ',ytrain.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(tokenizer,descs,train_feature,maxlen):\n",
    "    for desc in descs:\n",
    "        x,y,z = list(),list(),list()\n",
    "\n",
    "        for desc in descs:\n",
    "            seqofnumber = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1,len(seqofnumber)):\n",
    "                inseq = seqofnumber[:i]\n",
    "                outseq = seqofnumber[i]\n",
    "                inseq = pad_sequences([inseq],maxlen=maxlen)[0]\n",
    "\n",
    "                outseq = to_categorical([outseq],num_classes=train_vocab_size)[0]\n",
    "                y.append(inseq)\n",
    "                z.append(outseq)\n",
    "                x.append(train_feature)\n",
    "    return np.array(x),np.array(y),np.array(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#model.fit is occupying a lot of RAM because the entire dataset is fed to the model for training\n",
    "#model.fit_generator is used to feeding the dataset \n",
    "\n",
    "#In keras, fit() is much similar to sklearn's fit method, where you pass array of features as x values and target as y values.\n",
    "#You pass your whole dataset at once in fit method. Also, use it if you can load whole data into your memory (small dataset).\n",
    "\n",
    "#In fit_generator(), you don't pass the x and y directly, instead they come from a generator function. \n",
    "#This is for practical purpose, when you have large dataset. \n",
    "\n",
    "#so we create sequences for 1 photo at a time and then feed the sequence to model.fit_generator() train it to utilise less RAM.\n",
    "\n",
    "#refer for generator function and yield https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "#refer for fit and fit_generator https://datascience.stackexchange.com/questions/34444/what-is-the-difference-between-fit-and-fit-generator-in-keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mygenerator = (x*x for x in range(3))\n",
    "#type(mygenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tokenizer,train_descs,train_features,maxlen):\n",
    "    while 1:\n",
    "        for ids, descs in train_descs.items():\n",
    "            feature = train_features[ids][0]\n",
    "            feature_vector, inseq, outseq = create_sequence(tokenizer,descs,feature,maxlen)\n",
    "            yield[[feature_vector,inseq],outseq]\n",
    "generator = data_generator(tokenizer,train_descs,train_features,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs , outputs = next(data_generator(tokenizer,train_descs,train_features,maxlen))\n",
    "#print(inputs)\n",
    "#print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inputs[0])\n",
    "#print(inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 4096)\n",
      "(47, 33)\n",
      "(47, 7266)\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nepochs = 1\\nfor i in range(epochs):\\n    generator = data_generator(tokenizer,train_descs,train_features,maxlen)\\n    # fit for one epoch\\n    model.fit_generator(generator,epochs=1,verbose=1,steps_per_epoch=133)\\n    model.save('model_'+i+'.h5')\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "'''\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(tokenizer,train_descs,train_features,maxlen)\n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator,epochs=1,verbose=1,steps_per_epoch=133)\n",
    "    model.save('model_'+i+'.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to know about steps-per-epochs https://stackoverflow.com/questions/48604149/keras-fit-generator-and-steps-per-epoch\n",
    "#Refer to know about tokenizer https://github.com/keras-team/keras/issues/7551#issuecomment-322105443\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_for_id(4,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates description for a given image by iteratively appending next predicted word to the sentence. \n",
    "def generate_desc(feature_vector,tokenizer,model,maxlen):\n",
    "    sentence = 'startseq'\n",
    "    for i in range(maxlen):\n",
    "    \n",
    "        seqofnumber = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        seqofnumber = pad_sequences([seqofnumber],maxlen=maxlen)\n",
    "\n",
    "        predicted_vector = model.predict([feature_vector,seqofnumber],verbose=0)\n",
    "\n",
    "        indexed_word = argmax(predicted_vector)\n",
    "        word = word_for_id(indexed_wor,tokenizer)\n",
    "        \n",
    "        if word == None:\n",
    "            break\n",
    "        sentence = sentence + ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_desc(,tokenizer,model,maxlen)\n",
    "def p():\n",
    "    print(\"imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sdf', 'sd', 'r'], ['ioerm', 'cscan', 'vd'], ['pqowa', 'lsmx', 'cvx']]\n"
     ]
    }
   ],
   "source": [
    "s = ['sdf sd r','ioerm cscan vd','pqowa lsmx cvx']\n",
    "\n",
    "r = [d.split() for d in s]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world', 'this', 'is']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_caption = \"hello world this is\"\n",
    "predicted = list()\n",
    "predicted.append(pred_caption.split())\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'description' in link mlmastery = 'mapping' in this code which are dictionaries of {id:[desc1,desc2,desc3]}\n",
    "#check for bleu_score parameters https://stackoverflow.com/questions/40542523/nltk-corpus-level-bleu-vs-sentence-level-bleu-score\n",
    "'''\n",
    "see the no. associated with the below code lines\n",
    "#actual codeblock\n",
    "[#3\n",
    "    [#1              #many sentences for 1 image\n",
    "        [__,__,__],#2 #these 3 are different reference sentences for just 1 image(many sentences mapped with 1 imageid)\n",
    "        [__,__,__],#2\n",
    "        [__,__,__] #2\n",
    "    ],\n",
    "    \n",
    "    [ #1            #many sentences for 2nd image\n",
    "        [__,__,__],#2\n",
    "        [__,__,__],#2\n",
    "        [__,__,__] #2\n",
    "    ]#refer\n",
    "]#actual codeblock ends\n",
    "\n",
    "#predicted codeblock\n",
    "[#0\n",
    "    [__,__,__],#1st predicted sentence #5\n",
    "    [__,__,__],#2nd predicted sentence #5\n",
    "    [__,__,__],#3rd predicted sentence #5\n",
    "]\n",
    "\n",
    "'''\n",
    "\n",
    "def evaluate_model(mapping,tokenizer,maxlen,model,feature_vector):\n",
    "    actual,predicted = list(),list()#0\n",
    "    for ids,descs in mapping.items():#1\n",
    "        pred_caption = generate_desc(feature_vector[ids],tokenizer,model,maxlen)#caption string returned \n",
    "        for desc in descs:#2\n",
    "            reference.append(desc.split())\n",
    "        actual.append(reference)#3\n",
    "        predicted.append(pred_caption.split())#5\n",
    "        \n",
    "        #calculate BLEU scores for 1, 2, 3 and 4 cumulative n-grams.\n",
    "        #A higher score close to 1.0 is better, a score closer to zero is worse.\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "#BLEU scores are used in text translation for evaluating translated text against one or more reference translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mapping,tokenizer,maxlen,model,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "desc_list = [\"hi can you run\",\"DJ Bravo is here\",\"his name is gb\"] \n",
    "references = [d.split() for d in desc_list]\n",
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another alternative of doing the above task\n",
    "alternative = list()\n",
    "for desc in desc_list:\n",
    "        alternative.append(desc.split())\n",
    "print(alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mapping))\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mapping['1000268201_693b08cb0e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features['1000268201_693b08cb0e'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['1000268201_693b08cb0e'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside model.fit [X1train,X2train] X1train is numpy-array and X2train is numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When we call # evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "\n",
    "we know that test_features  which is a dictionary and contains id and a .\n",
    "ie - { id1: [ [ 4096 elements] ], id2: [ [ 4096 elements] ] }.\n",
    "\n",
    "Inside evaluate_model() we call -  generate_desc(model, tokenizer, photos[key], max_length)\n",
    "\n",
    "where photos[key] is numpy.ndarray and hold the 4096 elements in another numpy.array.\n",
    "\n",
    "To prove this I ran:\n",
    "Note: feature is same as photo here, which contain id and its corresponding 4096 vector.\n",
    "\n",
    "print(type(features['1000268201_693b08cb0e']))\n",
    "print(type(features['1000268201_693b08cb0e'][0])\n",
    "print(features['1000268201_693b08cb0e'].shape)\n",
    "print(features['1000268201_693b08cb0e'])\n",
    "\n",
    "and it gives me this output :\n",
    "\n",
    "numpy.ndarray\n",
    "numpy.ndarray\n",
    "(1, 4096)\n",
    "array([[ 2.5076463, -0.       , -0.       , ..., -0.       , -0.       ,\n",
    "        -0.       ]], dtype=float32)\n",
    "\n",
    "\n",
    "yhat = model.predict([photo,sequence], verbose=0) \n",
    "\n",
    "here photo[key] is a  np-array [ [ 4096 features inside ] ]. ie- array inside array.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reality\n",
    "photo\n",
    "[\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In generate_desc, photo is photo[key]. ie since photo = { id1: [ [ 4096 elements] ], id2: [ [ 4096 elements] ] }\n",
    "                                            So, photo[key] = [ [ 4096 elements ] ]\n",
    "so, photo = [ [ 4096 elements ] ]\n",
    "\n",
    "so, during call to model.predict() inside generate_desc()\n",
    "we pass [photo,sequence] and hence photo which is [ [ 4096 elements ] ] is correctly passed because during \n",
    "        model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "        if we print generator we get [x,y] where a is [x1,x2]. \n",
    "        y is next word to predict and x1 is photo, x2 is sequence of past encountered word.\n",
    "        To know what is x1,x2(ie-their type,shape). \n",
    "        we go into data_generator(), and we can see that it yields/returns [[in_img, in_seq], out_word]\n",
    "        and see that in_img,in_seq,out_word are returned by create_sequences\n",
    "        create_sequences() returns np.array(list()),np.array(list()),np.array(list())\n",
    "        In first, each element is an array and the array contain 4096 elements  \n",
    "        In second in_img, each element is an array(array of numbers each no. is token_no of a word encountered in previous steps) which can be seen by output  \n",
    "        In third out_img, each element is a array((initially a number but now an array in which 1 is present at index x where x is number from string seq[i]).\n",
    " \n",
    "                X1.append(train_features[key][0])#train_features is a dictionary and not a list.\n",
    "                #train_features = {'id' : [], 'id': []}\n",
    "                X2.append(inseq)\n",
    "                y.append(outseq)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filehandler = open('tokenizer.pkl','wb')\n",
    "dump(tokenizer,token_filehandler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
